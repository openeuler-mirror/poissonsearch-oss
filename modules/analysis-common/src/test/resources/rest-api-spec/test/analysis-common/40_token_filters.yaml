## Smoke tests for token filters included in the analysis-common module

"asciifolding":
    - do:
        indices.analyze:
          body:
            text:      Mus√©e d'Orsay
            tokenizer: keyword
            filter:    [asciifolding]
    - length: { tokens: 1 }
    - match:  { tokens.0.token: Musee d'Orsay }

---
"lowercase":
    - do:
        indices.analyze:
          body:
            text:      Foo Bar!
            tokenizer: keyword
            filter:    [lowercase]
    - length: { tokens: 1 }
    - match:  { tokens.0.token: foo bar! }

---
"word_delimiter":
    - do:
        indices.analyze:
          body:
            text:      the qu1ck brown fox
            tokenizer: standard
            filter:    [word_delimiter]
    - length: { tokens: 6 }
    - match:  { tokens.0.token: the }
    - match:  { tokens.1.token: qu }
    - match:  { tokens.2.token: "1" }
    - match:  { tokens.3.token: ck }
    - match:  { tokens.4.token: brown }
    - match:  { tokens.5.token: fox }

    - do:
        indices.analyze:
          body:
            text:      the qu1ck brown fox
            tokenizer: standard
            filter:
              - type: word_delimiter
                split_on_numerics: false
    - length: { tokens: 4 }
    - match:  { tokens.0.token: the }
    - match:  { tokens.1.token: qu1ck }
    - match:  { tokens.2.token: brown }
    - match:  { tokens.3.token: fox }

---
"word_delimiter_graph":
    - do:
        indices.analyze:
          body:
            text:      the qu1ck brown fox
            tokenizer: standard
            filter:    [word_delimiter_graph]
    - length: { tokens: 6 }
    - match:  { tokens.0.token: the }
    - match:  { tokens.1.token: qu }
    - match:  { tokens.2.token: "1" }
    - match:  { tokens.3.token: ck }
    - match:  { tokens.4.token: brown }
    - match:  { tokens.5.token: fox }

    - do:
        indices.analyze:
          body:
            text:      the qu1ck brown fox
            tokenizer: standard
            filter:
              - type: word_delimiter_graph
                split_on_numerics: false
    - length: { tokens: 4 }
    - match:  { tokens.0.token: the }
    - match:  { tokens.1.token: qu1ck }
    - match:  { tokens.2.token: brown }
    - match:  { tokens.3.token: fox }

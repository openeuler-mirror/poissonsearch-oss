/*
 * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
 * or more contributor license agreements. Licensed under the Elastic License;
 * you may not use this file except in compliance with the Elastic License.
 */
package org.elasticsearch.xpack.prelert.job.persistence;

import org.apache.logging.log4j.message.ParameterizedMessage;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.action.admin.indices.refresh.RefreshRequest;
import org.elasticsearch.action.bulk.BulkRequestBuilder;
import org.elasticsearch.action.bulk.BulkResponse;
import org.elasticsearch.action.index.IndexResponse;
import org.elasticsearch.client.Client;
import org.elasticsearch.common.bytes.BytesReference;
import org.elasticsearch.common.component.AbstractComponent;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.common.xcontent.ToXContent;
import org.elasticsearch.common.xcontent.XContentBuilder;
import org.elasticsearch.xpack.prelert.job.Job;
import org.elasticsearch.xpack.prelert.job.ModelSizeStats;
import org.elasticsearch.xpack.prelert.job.ModelSnapshot;
import org.elasticsearch.xpack.prelert.job.quantiles.Quantiles;
import org.elasticsearch.xpack.prelert.job.results.AnomalyRecord;
import org.elasticsearch.xpack.prelert.job.results.Bucket;
import org.elasticsearch.xpack.prelert.job.results.BucketInfluencer;
import org.elasticsearch.xpack.prelert.job.results.CategoryDefinition;
import org.elasticsearch.xpack.prelert.job.results.Influencer;
import org.elasticsearch.xpack.prelert.job.results.ModelDebugOutput;
import org.elasticsearch.xpack.prelert.job.results.ReservedFieldNames;

import java.io.IOException;
import java.util.Date;
import java.util.List;
import java.util.Map;
import java.util.function.Supplier;

import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;

/**
 * Saves result Buckets and Quantiles to Elasticsearch<br>
 *
 * <b>Buckets</b> are written with the following structure:
 * <h2>Bucket</h2> The results of each job are stored in buckets, this is the
 * top level structure for the results. A bucket contains multiple anomaly
 * records. The anomaly score of the bucket may not match the summed score of
 * all the records as all the records may not have been outputted for the
 * bucket.
 * <h2>Anomaly Record</h2> In Elasticsearch records have a parent &lt;-&lt;
 * child relationship with buckets and should only exist is relation to a parent
 * bucket. Each record was generated by a detector which can be identified via
 * the detectorIndex field.
 * <h2>Detector</h2> The Job has a fixed number of detectors but there may not
 * be output for every detector in each bucket. <br>
 * <b>Quantiles</b> may contain model quantiles used in normalisation and are
 * stored in documents of type {@link Quantiles#TYPE} <br>
 * <h2>ModelSizeStats</h2> This is stored in a flat structure <br>
 *
 * @see org.elasticsearch.xpack.prelert.job.persistence.ElasticsearchMappings
 */
public class JobResultsPersister extends AbstractComponent {

    private final Client client;

    public JobResultsPersister(Settings settings, Client client) {
        super(settings);
        this.client = client;
    }

    /**
     * Persist the result bucket
     */
    public void persistBucket(Bucket bucket) {
        if (bucket.getRecords() == null)  {
            return;
        }

        String jobId = bucket.getJobId();
        try {
            XContentBuilder content = serialiseWithJobId(Bucket.TYPE.getPreferredName(), bucket);
            String indexName = getJobIndexName(jobId);
            logger.trace("[{}] ES API CALL: index type {} to index {} at epoch {}", jobId, Bucket.TYPE, indexName, bucket.getEpoch());
            IndexResponse response = client.prepareIndex(indexName, Bucket.TYPE.getPreferredName())
                    .setSource(content)
                    .execute().actionGet();
            bucket.setId(response.getId());
            persistBucketInfluencersStandalone(jobId, bucket.getId(), bucket.getBucketInfluencers(), bucket.getTimestamp(),
                    bucket.isInterim());
            if (bucket.getInfluencers() != null && bucket.getInfluencers().isEmpty() == false) {
                BulkRequestBuilder addInfluencersRequest = client.prepareBulk();
                for (Influencer influencer : bucket.getInfluencers()) {
                    influencer.setTimestamp(bucket.getTimestamp());
                    influencer.setInterim(bucket.isInterim());
                    content = serialiseWithJobId(Influencer.TYPE.getPreferredName(), influencer);
                    logger.trace("[{}] ES BULK ACTION: index type {} to index {} with auto-generated ID",
                            jobId, Influencer.TYPE, indexName);
                    addInfluencersRequest.add(client.prepareIndex(indexName, Influencer.TYPE.getPreferredName())
                                    .setSource(content));
                }
                logger.trace("[{}] ES API CALL: bulk request with {} actions", jobId, addInfluencersRequest.numberOfActions());
                BulkResponse addInfluencersResponse = addInfluencersRequest.execute().actionGet();
                if (addInfluencersResponse.hasFailures()) {
                    logger.error("[{}] Bulk index of Influencers has errors: {}", jobId, addInfluencersResponse.buildFailureMessage());
                }
            }
            if (bucket.getRecords().isEmpty() == false) {
                BulkRequestBuilder addRecordsRequest = client.prepareBulk();
                for (AnomalyRecord record : bucket.getRecords()) {
                    record.setTimestamp(bucket.getTimestamp());
                    content = serialiseWithJobId(AnomalyRecord.TYPE.getPreferredName(), record);
                    logger.trace("[{}] ES BULK ACTION: index type {} to index {} with auto-generated ID, for bucket {}",
                            jobId, AnomalyRecord.TYPE, indexName, bucket.getId());
                    addRecordsRequest.add(client.prepareIndex(indexName, AnomalyRecord.TYPE.getPreferredName())
                            .setSource(content));
                }

                logger.trace("[{}] ES API CALL: bulk request with {} actions", jobId, addRecordsRequest.numberOfActions());
                BulkResponse addRecordsResponse = addRecordsRequest.execute().actionGet();
                if (addRecordsResponse.hasFailures()) {
                    logger.error("[{}] Bulk index of AnomalyRecord has errors: {}", jobId, addRecordsResponse.buildFailureMessage());
                }
            }
            persistPerPartitionMaxProbabilities(bucket);
        } catch (IOException e) {
            logger.error(new ParameterizedMessage("[{}] Error writing bucket state", new Object[] {jobId}, e));
        }
    }

    /**
     * Persist the category definition
     * @param category The category to be persisted
     */
    public void persistCategoryDefinition(CategoryDefinition category) {
        Persistable persistable = new Persistable(category.getJobId(), category, CategoryDefinition.TYPE::getPreferredName,
                () -> String.valueOf(category.getCategoryId()), () -> serialiseCategoryDefinition(category));
        persistable.persist();
        // Don't commit as we expect masses of these updates and they're not
        // read again by this process
    }

    /**
     * Persist the quantiles
     */
    public void persistQuantiles(Quantiles quantiles) {
        Persistable persistable = new Persistable(quantiles.getJobId(), quantiles, Quantiles.TYPE::getPreferredName,
                () -> Quantiles.QUANTILES_ID, () -> serialiseWithJobId(Quantiles.TYPE.getPreferredName(), quantiles));
        if (persistable.persist()) {
            // Refresh the index when persisting quantiles so that previously
            // persisted results will be available for searching.  Do this using the
            // indices API rather than the index API (used to write the quantiles
            // above), because this will refresh all shards rather than just the
            // shard that the quantiles document itself was written to.
            commitWrites(quantiles.getJobId());
        }
    }

    /**
     * Persist a model snapshot description
     */
    public void persistModelSnapshot(ModelSnapshot modelSnapshot) {
        Persistable persistable = new Persistable(modelSnapshot.getJobId(), modelSnapshot, ModelSnapshot.TYPE::getPreferredName,
                modelSnapshot::getSnapshotId, () -> serialiseWithJobId(ModelSnapshot.TYPE.getPreferredName(), modelSnapshot));
        persistable.persist();
    }

    /**
     * Persist the memory usage data
     */
    public void persistModelSizeStats(ModelSizeStats modelSizeStats) {
        String jobId = modelSizeStats.getJobId();
        logger.trace("[{}] Persisting model size stats, for size {}", jobId, modelSizeStats.getModelBytes());
        Persistable persistable = new Persistable(modelSizeStats.getJobId(), modelSizeStats, ModelSizeStats.TYPE::getPreferredName,
                () -> jobId, () -> serialiseWithJobId(ModelSizeStats.TYPE.getPreferredName(), modelSizeStats));
        persistable.persist();
        persistable = new Persistable(modelSizeStats.getJobId(), modelSizeStats, ModelSizeStats.TYPE::getPreferredName,
                () -> null, () -> serialiseWithJobId(ModelSizeStats.TYPE.getPreferredName(), modelSizeStats));
        persistable.persist();
        // Don't commit as we expect masses of these updates and they're only
        // for information at the API level
    }

    /**
     * Persist model debug output
     */
    public void persistModelDebugOutput(ModelDebugOutput modelDebugOutput) {
        Persistable persistable = new Persistable(modelDebugOutput.getJobId(), modelDebugOutput, ModelDebugOutput.TYPE::getPreferredName,
                () -> null, () -> serialiseWithJobId(ModelDebugOutput.TYPE.getPreferredName(), modelDebugOutput));
        persistable.persist();
        // Don't commit as we expect masses of these updates and they're not
        // read again by this process
    }

    /**
     * Persist the influencer
     */
    public void persistInfluencer(Influencer influencer) {
        Persistable persistable = new Persistable(influencer.getJobId(), influencer, Influencer.TYPE::getPreferredName,
                influencer::getId, () -> serialiseWithJobId(Influencer.TYPE.getPreferredName(), influencer));
        persistable.persist();
        // Don't commit as we expect masses of these updates and they're not
        // read again by this process
    }

    /**
     * Persist state sent from the native process
     */
    public void persistBulkState(String jobId, BytesReference bytesRef) {
        try {
            // No validation - assume the native process has formatted the state correctly
            byte[] bytes = bytesRef.toBytesRef().bytes;
            logger.trace("[{}] ES API CALL: bulk index", jobId);
            client.prepareBulk()
                    .add(bytes, 0, bytes.length)
                    .execute().actionGet();
        } catch (Exception e) {
            logger.error((org.apache.logging.log4j.util.Supplier<?>)
                    () -> new ParameterizedMessage("[{}] Error persisting bulk state", jobId), e);
        }
    }

    /**
     * Delete any existing interim results
     */
    public void deleteInterimResults(String jobId) {
        ElasticsearchBulkDeleter deleter = new ElasticsearchBulkDeleter(client, jobId, true);
        deleter.deleteInterimResults();
        deleter.commit(new ActionListener<BulkResponse>() {
            @Override
            public void onResponse(BulkResponse bulkResponse) {
                // don't care?
            }

            @Override
            public void onFailure(Exception e) {
                // don't care?
            }
        });
    }

    /**
     * Once all the job data has been written this function will be
     * called to commit the data if the implementing persister requires
     * it.
     *
     * @return True if successful
     */
    public boolean commitWrites(String jobId) {
        String indexName = getJobIndexName(jobId);
        // Refresh should wait for Lucene to make the data searchable
        logger.trace("[{}] ES API CALL: refresh index {}", jobId, indexName);
        client.admin().indices().refresh(new RefreshRequest(indexName)).actionGet();
        return true;
    }


    XContentBuilder serialiseWithJobId(String objField, ToXContent obj) throws IOException {
        XContentBuilder builder = jsonBuilder();
        obj.toXContent(builder, ToXContent.EMPTY_PARAMS);
        return builder;
    }

    private XContentBuilder serialiseCategoryDefinition(CategoryDefinition categoryDefinition) throws IOException {
        XContentBuilder builder = jsonBuilder();
        categoryDefinition.toXContent(builder, ToXContent.EMPTY_PARAMS);
        return builder;
    }

    void persistBucketInfluencersStandalone(String jobId, String bucketId, List<BucketInfluencer> bucketInfluencers,
                                                    Date bucketTime, boolean isInterim) throws IOException {
        if (bucketInfluencers != null && bucketInfluencers.isEmpty() == false) {
            BulkRequestBuilder addBucketInfluencersRequest = client.prepareBulk();
            for (BucketInfluencer bucketInfluencer : bucketInfluencers) {
                XContentBuilder content = serialiseBucketInfluencerStandalone(bucketInfluencer, bucketTime, isInterim);
                // Need consistent IDs to ensure overwriting on renormalisation
                String id = bucketId + bucketInfluencer.getInfluencerFieldName();
                String indexName = getJobIndexName(jobId);
                logger.trace("[{}] ES BULK ACTION: index type {} to index {} with ID {}", jobId, BucketInfluencer.TYPE, indexName, id);
                addBucketInfluencersRequest.add(
                        client.prepareIndex(indexName, BucketInfluencer.TYPE.getPreferredName(), id)
                                .setSource(content));
            }
            logger.trace("[{}] ES API CALL: bulk request with {} actions", jobId, addBucketInfluencersRequest.numberOfActions());
            BulkResponse addBucketInfluencersResponse = addBucketInfluencersRequest.execute().actionGet();
            if (addBucketInfluencersResponse.hasFailures()) {
                logger.error("[{}] Bulk index of Bucket Influencers has errors: {}", jobId,
                        addBucketInfluencersResponse.buildFailureMessage());
            }
        }
    }

    private XContentBuilder serialiseBucketInfluencerStandalone(BucketInfluencer bucketInfluencer,
                                                                Date bucketTime, boolean isInterim) throws IOException {
        BucketInfluencer influencer = new BucketInfluencer(bucketInfluencer);
        influencer.setIsInterim(isInterim);
        influencer.setTimestamp(bucketTime);
        XContentBuilder builder = jsonBuilder();
        influencer.toXContent(builder, ToXContent.EMPTY_PARAMS);
        return builder;
    }

    void persistPerPartitionMaxProbabilities(Bucket bucket) {
        String jobId = bucket.getJobId();
        if (bucket.getPerPartitionMaxProbability().isEmpty()) {
            return;
        }

        try {
            XContentBuilder builder = jsonBuilder();
            builder.startObject()
                    .field(ElasticsearchMappings.ES_TIMESTAMP, bucket.getTimestamp())
                    .field(Job.ID.getPreferredName(), bucket.getJobId());
            builder.startArray(ReservedFieldNames.PARTITION_NORMALIZED_PROBS);
            for (Map.Entry<String, Double> entry : bucket.getPerPartitionMaxProbability().entrySet()) {
                builder.startObject()
                        .field(AnomalyRecord.PARTITION_FIELD_VALUE.getPreferredName(), entry.getKey())
                        .field(Bucket.MAX_NORMALIZED_PROBABILITY.getPreferredName(), entry.getValue())
                        .endObject();
            }
            builder.endArray().endObject();
            String indexName = getJobIndexName(jobId);
            logger.trace("[{}] ES API CALL: index type {} to index {} at epoch {}",
                    jobId, ReservedFieldNames.PARTITION_NORMALIZED_PROB_TYPE, indexName, bucket.getEpoch());
            client.prepareIndex(indexName, ReservedFieldNames.PARTITION_NORMALIZED_PROB_TYPE)
                    .setSource(builder)
                    .setId(bucket.getId())
                    .execute().actionGet();
        } catch (IOException e) {
            logger.error(new ParameterizedMessage("[{}] error updating bucket per partition max normalized scores",
                    new Object[]{jobId}, e));
        }
    }

    private static final String INDEX_PREFIX = "prelertresults-";

    public static String getJobIndexName(String jobId) {
        return INDEX_PREFIX + jobId;
    }

    private class Persistable {

        private final String jobId;
        private final Object object;
        private final Supplier<String> typeSupplier;
        private final Supplier<String> idSupplier;
        private final Serialiser serialiser;

        Persistable(String jobId, Object object, Supplier<String> typeSupplier, Supplier<String> idSupplier,
                    Serialiser serialiser) {
            this.jobId = jobId;
            this.object = object;
            this.typeSupplier = typeSupplier;
            this.idSupplier = idSupplier;
            this.serialiser = serialiser;
        }

        boolean persist() {
            String type = typeSupplier.get();
            String id = idSupplier.get();

            if (object == null) {
                logger.warn("[{}] No {} to persist for job ", jobId, type);
                return false;
            }

            logCall(type, id);

            try {
                String indexName = getJobIndexName(jobId);
                client.prepareIndex(indexName, type, idSupplier.get())
                        .setSource(serialiser.serialise())
                        .execute().actionGet();
                return true;
            } catch (IOException e) {
                logger.error(new ParameterizedMessage("[{}] Error writing {}", new Object[]{jobId, typeSupplier.get()}, e));
                return false;
            }
        }

        private void logCall(String type, String id) {
            String indexName = getJobIndexName(jobId);
            if (id != null) {
                logger.trace("[{}] ES API CALL: index type {} to index {} with ID {}", jobId, type, indexName, id);
            } else {
                logger.trace("[{}] ES API CALL: index type {} to index {} with auto-generated ID", jobId, type, indexName);
            }
        }
    }

    private interface Serialiser {
        XContentBuilder serialise() throws IOException;
    }
}
